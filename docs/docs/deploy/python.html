





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API and Gradio Frontend &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="iOS App and Swift API" href="ios.html" />
    <link rel="prev" title="CLI and C++ API" href="cli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://mlc.ai/blog>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/try_out.html">Try out MLC Chat</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI and C++ API</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API and Gradio Frontend</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#python-api">Python API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#api-reference">API Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mlc_chat.ChatModule"><code class="docutils literal notranslate"><span class="pre">ChatModule</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.__init__"><code class="docutils literal notranslate"><span class="pre">ChatModule.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.benchmark_generate"><code class="docutils literal notranslate"><span class="pre">ChatModule.benchmark_generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.embed_text"><code class="docutils literal notranslate"><span class="pre">ChatModule.embed_text()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.generate"><code class="docutils literal notranslate"><span class="pre">ChatModule.generate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.reset_chat"><code class="docutils literal notranslate"><span class="pre">ChatModule.reset_chat()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_chat.ChatModule.stats"><code class="docutils literal notranslate"><span class="pre">ChatModule.stats()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#gradio-frontend">Gradio Frontend</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API and Gradio Frontend</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api-and-gradio-frontend">
<h1>Python API and Gradio Frontend<a class="headerlink" href="#python-api-and-gradio-frontend" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#python-api" id="id1">Python API</a></p>
<ul>
<li><p><a class="reference internal" href="#verify-installation" id="id2">Verify Installation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#api-reference" id="id3">API Reference</a></p></li>
<li><p><a class="reference internal" href="#gradio-frontend" id="id4">Gradio Frontend</a></p></li>
</ul>
</nav>
<p>We expose Python API for the MLC-Chat for easy integration into other Python projects,
we also provide a web demo based on <a class="reference external" href="https://gradio.app/">gradio</a> as an example of using Python API to interact with MLC-Chat.</p>
<section id="python-api">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Python API</a><a class="headerlink" href="#python-api" title="Permalink to this heading">¶</a></h2>
<p>The Python API is a part of the MLC-Chat package, which we have prepared pre-built pip wheels and you can install it by
following the instructions in <a class="reference external" href="https://mlc.ai/package/">https://mlc.ai/package/</a>.</p>
<section id="verify-installation">
<h3><a class="toc-backref" href="#id2" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_chat import ChatModule; print(ChatModule)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the information about the <code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code> class.</p>
<p>If the prebuild is unavailable on your platform, or you would like to build a runtime
that supports other GPU runtime than the prebuilt version. Please refer our <a class="reference internal" href="rest.html#mlcchat-package-build-from-source"><span class="std std-ref">Build MLC-Chat Package From Source</span></a> tutorial.</p>
</section>
</section>
<section id="api-reference">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h2>
<p>User can initiate a chat module by creating <code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code> class, which is a wrapper of the MLC-Chat model.
The <code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code> class provides the following methods:</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlc_chat.ChatModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_chat.</span></span><span class="sig-name descname"><span class="pre">ChatModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize a chat module.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>str</em>) – The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p></li>
<li><p><strong>device_name</strong> (<em>str</em>) – The device name, enter one of “cuda”, “metal”, “vulkan”, “rocm”, “opencl”, “auto”.
If “auto”, the local device will be automatically detected.</p></li>
<li><p><strong>device_id</strong> (<em>int</em>) – The device id passed to <code class="docutils literal notranslate"><span class="pre">tvm</span></code>.</p></li>
<li><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><em>ChatConfig</em><em>]</em>) – A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. Will be used to override the
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p></li>
<li><p><strong>lib_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.benchmark_generate">
<span class="sig-name descname"><span class="pre">benchmark_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.benchmark_generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Given the input prompt and target length of generation,
generate text with performance benchmark. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_chat.chat_module</span> <span class="kn">import</span> <span class="n">ChatModule</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">benchmark_generate</span><span class="p">(</span><span class="s2">&quot;What&#39;s the meaning of life?&quot;</span><span class="p">,</span> <span class="n">generate_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated text:</span><span class="se">\n</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>will generate 256 tokens in total based on prompt “What’s the meaning
of life?”. After generation, you can use <cite>cm.stats()</cite> to print the
generation speed.</p>
<p class="rubric">Notes</p>
<p>1. This method generates text without system prompt (i.e., it is pure
text generation with no chat style).
2. To generate text until the target generation length, this method
ignores the stop token of the model. This means text generated after the
stop token might be meaningless.
3. To make the benchmark as accurate as possible, we first do a round of
prefill and decode before text generation.
4. This method resets the previous performance statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) – The prompt of the text generation.</p></li>
<li><p><strong>generate_length</strong> (<em>int</em>) – The target length of generation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> – The generated text output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.embed_text">
<span class="sig-name descname"><span class="pre">embed_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.embed_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text input, returns its embedding in the LLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>str</em>) – The user input string.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>embedding</strong> – The embedding of the text.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tvm.runtime.NDArray</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a high-level method and is only used for retrieving text embeddings. Users are
not supposed to call <a class="reference internal" href="#mlc_chat.ChatModule.generate" title="mlc_chat.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> after calling this method in the same chat session,
since the input to this method is not prefilled and will cause error. If user needs to
call <a class="reference internal" href="#mlc_chat.ChatModule.generate" title="mlc_chat.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> later, please call <a class="reference internal" href="#mlc_chat.ChatModule.reset_chat" title="mlc_chat.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a> first.
For a more fine-grained embedding API, see <code class="xref py py-func docutils literal notranslate"><span class="pre">_embed()</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt:</span> <span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_callback=&lt;mlc_chat.callback.stream_to_stdout</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>A high-level method that generates the response from the chat module given a user prompt.
User can specify which callback method to use upon receiving the response.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) – The user input prompt, i.e. a question to ask the chat module.</p></li>
<li><p><strong>progress_callback</strong> (<em>object</em>) – Optional argument. The callback method used upon receiving the response from the chat module.
User should pass in a callback class. See <cite>mlc_chat/callback.py</cite> for a full list
of available callback classes. By default, the response is streamed to stdout.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The generate api gives the raw response, and no chat bot role name will be displayed
prior to the response. User can retrieve the role name via <code class="xref py py-func docutils literal notranslate"><span class="pre">_get_role_1()</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.reset_chat">
<span class="sig-name descname"><span class="pre">reset_chat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatConfig</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_chat.ChatModule.reset_chat" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the chat session, clear all chat history, and potentially
override the original <cite>mlc-chat-config.json</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><em>ChatConfig</em><em>]</em>) – A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. If specified, the chat
module will reload the <cite>mlc-chat-config.json</cite>, and override it with
<code class="docutils literal notranslate"><span class="pre">chat_config</span></code>, just like in initialization.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model remains the same after <a class="reference internal" href="#mlc_chat.ChatModule.reset_chat" title="mlc_chat.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a>.
To reload module, please either re-initialize a <a class="reference internal" href="#mlc_chat.ChatModule" title="mlc_chat.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code></a> instance
or use <code class="xref py py-func docutils literal notranslate"><span class="pre">_reload()</span></code> instead.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_chat.ChatModule.stats">
<span class="sig-name descname"><span class="pre">stats</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_chat.ChatModule.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the runtime stats of the encoding step, decoding step, (and embedding step if exists)
of the chat module in text form.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stats</strong> – The runtime stats text.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="gradio-frontend">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Gradio Frontend</a><a class="headerlink" href="#gradio-frontend" title="Permalink to this heading">¶</a></h2>
<p>The gradio frontend provides a web interface for the MLC-Chat model, which allows user to interact with the model in a more user-friendly way.
To use gradio frontend, you need to install gradio first:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>gradio
</pre></div>
</div>
<p>Then you can run the following code to start the interface:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>mlc_chat.gradio<span class="w"> </span>--artifact-path<span class="w"> </span>ARTIFACT_PATH<span class="w"> </span>--device-name<span class="w"> </span>DEVICE_NAME<span class="w"> </span>--device-id<span class="w"> </span>DEVICE_ID<span class="w"> </span><span class="o">[</span>--port<span class="w"> </span>PORT_NUMBER<span class="o">]</span><span class="w"> </span><span class="o">[</span>--share<span class="o">]</span>
</pre></div>
</div>
<dl class="option-list">
<dt><kbd><span class="option">--artifact-path</span></kbd></dt>
<dd><p>The path to the artifact folder where models are stored. The default value is <code class="docutils literal notranslate"><span class="pre">dist</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device-name</span></kbd></dt>
<dd><p>The device name to run the model. Available options are:
<code class="docutils literal notranslate"><span class="pre">metal</span></code>, <code class="docutils literal notranslate"><span class="pre">cuda</span></code>, <code class="docutils literal notranslate"><span class="pre">vulkan</span></code>, <code class="docutils literal notranslate"><span class="pre">cpu</span></code>. The default value is <code class="docutils literal notranslate"><span class="pre">cuda</span></code>.</p>
</dd>
<dt><kbd><span class="option">--device-id</span></kbd></dt>
<dd><p>The device id to run the model. The default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</dd>
<dt><kbd><span class="option">--port</span></kbd></dt>
<dd><p>The port number to run gradio. The default value is <code class="docutils literal notranslate"><span class="pre">7860</span></code>.</p>
</dd>
<dt><kbd><span class="option">--share</span></kbd></dt>
<dd><p>Whether to create a publicly shareable link for the interface.</p>
</dd>
</dl>
<p>After setting up properly, you are expected to see the following interface in your browser:</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/gradio-interface.png" style="width: 100%;" /></a>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ios.html" class="btn btn-neutral float-right" title="iOS App and Swift API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cli.html" class="btn btn-neutral float-left" title="CLI and C++ API" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>