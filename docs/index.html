<!DOCTYPE html>
<html>
    <head>
        <title>MLC LLM | Home</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
              integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
              crossorigin="anonymous">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="/assets/css/main.css">
        <link rel="stylesheet" href="/assets/css/group.css">
        <!-- <link rel="stylesheet" href="/css/table.css">          -->
        <link rel="shortcut icon" href="/assets/img/logo/mlc-favicon.png">
        
    </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
            <div class="header d-flex
                        flex-column
                        flex-md-row justify-content-md-between">
              <a href="/" id="navtitle">
                  <img src="/assets/img/logo/mlc-logo-with-text-landscape.svg" height="70px"
                       alt="MLC" id="logo">
              </a>
              <ul id="topbar" class="nav nav-pills justify-content-center">
                    
                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link active"
                               href="/">
                                Home
                            </a>
                         
                        </li>

                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link "
                               href="/docs">
                                Docs
                            </a>
                         
                        </li>

                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link "
                               href="https://github.com/mlc-ai/mlc-llm">
                                Github
                            </a>
                         
                        </li>

                    

                </ul>
            </div>

            

            
            <!-- Schedule  -->
            
                <h1 id="mlc-llm">MLC LLM</h1>

<table>
  <tbody>
    <tr>
      <td><a href="https://llm.mlc.ai/docs">Documentation</a></td>
      <td><a href="https://blog.mlc.ai/">Blog</a></td>
      <td>[Discord][discord-url]</td>
    </tr>
  </tbody>
</table>

<p><strong>M</strong>achine <strong>L</strong>earning <strong>C</strong>ompilation for <strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odels (MLC LLM) is a high-performance universal deployment solution that allows native deployment of any large language models with native APIs with compiler acceleration. The mission of this project is to enable everyone to develop, optimize and deploy AI models natively on everyone’s devices with ML compilation techniques.</p>

<p><strong>Universal deployment.</strong> MLC LLM supports the following platforms and hardware:</p>

<table style="width:100%">
  <thead>
    <tr>
      <th style="width:15%"> </th>
      <th style="width:20%">AMD GPU</th>
      <th style="width:20%">NVIDIA GPU</th>
      <th style="width:20%">Apple GPU</th>
      <th style="width:24%">Intel GPU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Linux / Win</td>
      <td>✅ Vulkan, ROCm</td>
      <td>✅ Vulkan, CUDA</td>
      <td>N/A</td>
      <td>✅ Vulkan</td>
    </tr>
    <tr>
      <td>macOS</td>
      <td>✅ Metal (dGPU)</td>
      <td>N/A</td>
      <td>✅ Metal</td>
      <td>✅ Metal (iGPU)</td>
    </tr>
    <tr>
      <td>Web Browser</td>
      <td colspan="4">✅ WebGPU and WASM </td>
    </tr>
    <tr>
      <td>iOS / iPadOS</td>
      <td colspan="4">✅ Metal on Apple A-series GPU</td>
    </tr>
    <tr>
      <td>Android</td>
      <td colspan="2">✅ OpenCL on Adreno GPU</td>
      <td colspan="2">✅ OpenCL on Mali GPU</td>
    </tr>
  </tbody>
</table>

<h2 id="quick-start">Quick Start</h2>

<p>We introduce the quick start examples of chat CLI, Python API and REST server here to use MLC LLM.
We use 4-bit quantized 8B Llama-3 model for demonstration purpose.
The pre-quantized Llama-3 weights is available at https://huggingface.co/mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC.
You can also try out unquantized Llama-3 model by replacing <code class="language-plaintext highlighter-rouge">q4f16_1</code> to <code class="language-plaintext highlighter-rouge">q0f16</code> in the examples below.
Please visit our <a href="https://llm.mlc.ai/docs/index.html">documentation</a> for detailed quick start and introduction.</p>

<h3 id="installation">Installation</h3>

<p>MLC LLM is available via <a href="https://llm.mlc.ai/docs/install/mlc_llm.html#install-mlc-packages">pip</a>.
It is always recommended to install it in an isolated conda virtual environment.</p>

<p>To verify the installation, activate your virtual environment, run</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python <span class="nt">-c</span> <span class="s2">"import mlc_llm; print(mlc_llm.__path__)"</span>
</code></pre></div></div>

<p>You are expected to see the installation path of MLC LLM Python package.</p>

<h3 id="chat-cli">Chat CLI</h3>

<p>We can try out the chat CLI in MLC LLM with 4-bit quantized 8B Llama-3 model.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mlc_llm chat HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC
</code></pre></div></div>

<p>It may take 1-2 minutes for the first time running this command.
After waiting, this command launch a chat interface where you can enter your prompt and chat with the model.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You can use the following special commands:
/help               print the special commands
/exit               quit the cli
/stats              print out the latest stats (token/sec)
/reset              restart a fresh chat
/set [overrides]    override settings in the generation config. For example,
                      `/set temperature=0.5;max_gen_len=100;stop=end,stop`
                      Note: Separate stop words in the `stop` option with commas (,).
Multi-line input: Use escape+enter to start a new line.

user: What's the meaning of life
assistant:
What a profound and intriguing question! While there's no one definitive answer, I'd be happy to help you explore some perspectives on the meaning of life.

The concept of the meaning of life has been debated and...
</code></pre></div></div>

<h3 id="python-api">Python API</h3>

<p>We can run the Llama-3 model with the chat completion Python API of MLC LLM.
You can save the code below into a Python file and run it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">mlc_llm</span> <span class="kn">import</span> <span class="n">LLMEngine</span>

<span class="c1"># Create engine
</span><span class="n">model</span> <span class="o">=</span> <span class="sh">"</span><span class="s">HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC</span><span class="sh">"</span>
<span class="n">engine</span> <span class="o">=</span> <span class="nc">LLMEngine</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Run chat completion in OpenAI API.
</span><span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">engine</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">What is the meaning of life?</span><span class="sh">"</span><span class="p">}],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">choice</span><span class="p">.</span><span class="n">delta</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="sh">""</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="n">engine</span><span class="p">.</span><span class="nf">terminate</span><span class="p">()</span>
</code></pre></div></div>

<p><strong>The Python API of <code class="language-plaintext highlighter-rouge">mlc_llm.LLMEngine</code> fully aligns with OpenAI API</strong>.
You can use LLMEngine in the same way of using
<a href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
for both synchronous and asynchronous generation.</p>

<p>If you would like to do concurrent asynchronous generation, you can use <code class="language-plaintext highlighter-rouge">mlc_llm.AsyncLLMEngine</code> instead.</p>

<h3 id="rest-server">REST Server</h3>

<p>We can launch a REST server to serve the 4-bit quantized Llama-3 model for OpenAI chat completion requests.
The server has fully OpenAI API completeness.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mlc_llm serve HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC
</code></pre></div></div>

<p>The server is hooked at <code class="language-plaintext highlighter-rouge">http://127.0.0.1:8000</code> by default, and you can use <code class="language-plaintext highlighter-rouge">--host</code> and <code class="language-plaintext highlighter-rouge">--port</code>
to set a different host and port.
When the server is ready (showing <code class="language-plaintext highlighter-rouge">INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</code>),
we can open a new shell and send a cURL request via the following command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST <span class="se">\</span>
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{
        "model": "HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC",
        "messages": [
            {"role": "user", "content": "Hello! Our project is MLC LLM. What is the name of our project?"}
        ]
  }'</span> <span class="se">\</span>
  http://127.0.0.1:8000/v1/chat/completions
</code></pre></div></div>

<h2 id="universal-deployment-apis">Universal Deployment APIs</h2>

<p>MLC LLM provides multiple sets of APIs across platforms and environments. These include</p>
<ul>
  <li><a href="https://llm.mlc.ai/docs/deploy/python_engine.html">Python API</a></li>
  <li><a href="https://llm.mlc.ai/docs/deploy/rest.html">OpenAI-compatible Rest-API</a></li>
  <li><a href="https://llm.mlc.ai/docs/deploy/cli.html">C++ API</a></li>
  <li><a href="https://llm.mlc.ai/docs/deploy/javascript.html">JavaScript API</a> and <a href="https://github.com/mlc-ai/web-llm">Web LLM</a></li>
  <li><a href="https://llm.mlc.ai/docs/deploy/ios.html">Swift API for iOS App</a></li>
  <li><a href="https://llm.mlc.ai/docs/deploy/android.html">Java API and Android App</a></li>
</ul>

<h2 id="links">Links</h2>

<ul>
  <li>You might want to check out our online public <a href="https://mlc.ai">Machine Learning Compilation course</a> for a systematic
walkthrough of our approaches.</li>
  <li><a href="https://webllm.mlc.ai/">WebLLM</a> is a companion project using MLC LLM’s WebGPU and WebAssembly backend.</li>
  <li><a href="https://websd.mlc.ai/">WebStableDiffusion</a> is a companion project for diffusion models with the WebGPU backend.</li>
</ul>

<h2 id="disclaimer">Disclaimer</h2>

<p>The pre-packaged demos are subject to the model License.</p>

            
        </div> <!-- /container -->

        <!-- Support retina images. -->
        <script type="text/javascript"
                src="/assets/js/srcset-polyfill.js"></script>
    </body>

</html>
